{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f939aa7f-9987-416a-841e-5b66354283cc",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "#### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67770993-195c-44cc-a950-1a68038ced29",
   "metadata": {},
   "source": [
    "Now that we've collected our initial data, let's clean it up and work towards giving a target class label\n",
    "\n",
    "Table of Contents:<a id=\"Contents\"></a>\n",
    "* [Initial Cleaning](#initialcleaning)\n",
    "* [Finding Target Class](#targetclass)\n",
    "* [Scrape for Additional Data](#moreinfo)\n",
    "* [Conclusion & Next Steps](#nextsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7879442-f1b6-4240-bd55-24098d075301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312042b6-8c48-4dd2-a2c1-d19778f8da50",
   "metadata": {},
   "source": [
    "## Initial Cleaning<a id=\"initialcleaning\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6436c6-da66-497c-8892-a8574b9894eb",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51458b9-db59-4b66-87fd-f0da9365e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/discord_data.csv') # from discord ~ base metrics\n",
    "df2 = pd.read_csv('../data/ath_price_data.csv') # from dune ~ ath date and price\n",
    "df3 = pd.read_csv('../data/defined_data.csv') # from defined (token info only) ~ ticker and total supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5bf5e-7e2c-4f93-ae10-4ce568dd6ee0",
   "metadata": {},
   "source": [
    "Rename columns for merging ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d186fb-31d4-4881-adcd-a45a234d2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={'token_bought_address': 'address'})\n",
    "df2 = df2.rename(columns={'block_date': 'ath_date', 'price':'ath_price'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0808dc1-ffd9-483a-8bed-b2dc63265975",
   "metadata": {},
   "source": [
    "Ensure all addresses are completely lowercase for mergining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4d042a-57e6-4770-a817-c971ac8ba9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['address'] = df1['address'].str.lower()\n",
    "df2['address'] = df2['address'].str.lower()\n",
    "df3['address'] = df3['address'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f4b91-9f83-47dc-89be-f20d0345be99",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c1cd47-87a7-46b7-bcf0-7d3949f955eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_merge = pd.merge(df1, df2, on='address', how='outer')\n",
    "df = pd.merge(first_merge, df3, on='address', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b89e11-da96-42aa-b237-7360833f1ae6",
   "metadata": {},
   "source": [
    "Drop nulls and duplicate addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d25a158-495f-4a46-84eb-b3b87ae43de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset='address', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c83a5d-0276-451b-b213-528f8f8cf0b2",
   "metadata": {},
   "source": [
    "'created' column from discord timestamp to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6563a3-a3b9-4926-a341-29b3ab9b5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discord timestamp string to datetime object\n",
    "def convert_to_date(timestamp_string):\n",
    "    \n",
    "    # extract timestamp number\n",
    "    timestamp = int(timestamp_string.split(':')[1].split(':')[0])\n",
    "    \n",
    "    # Convert extracted timestamp to  datetime object with datetime library\n",
    "    date = datetime.datetime.fromtimestamp(timestamp)\n",
    "    return pd.to_datetime(date.date())\n",
    "\n",
    "# Apply the conversion function to the entire column\n",
    "df['created'] = df['created'].apply(convert_to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58898f7-7f7f-4982-9dae-6dd919c68329",
   "metadata": {},
   "source": [
    "Clean columns with non ml interpretable symbols to base numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb3ca47-4b41-404a-98ab-ab08ba3ff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ath_date'] = pd.to_datetime(df['ath_date'])\n",
    "df['funding_source_rating'] = df['funding_source'].str.extract(r':(\\w+)_circle:')\n",
    "df['funding_source'] = df['funding_source'].str.extract(r'\\[(.*?)\\]')\n",
    "df['max_wallet'] = df['max_wallet'].str.split('|').str.get(0).str.strip()\n",
    "df['max_tx'] = df['max_tx'].str.split('|').str.get(0).str.strip()\n",
    "df['funding_amount'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddb424-8ae1-405a-a6a1-0d1ca0d3463d",
   "metadata": {},
   "source": [
    "The following columns need to be made numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4edf82-15f2-4ec8-ac23-a2c38ce67c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numeric = ['buytax', 'selltax', 'max_wallet', 'max_tx']\n",
    "df = df.replace('-', np.nan)\n",
    "\n",
    "#In the discord source docs, if max_wallet has a value of 'TD' it simply means theres a transfer delay and the max value is the same as max_tx\n",
    "df.loc[df['max_wallet'] == 'TD', 'max_wallet'] = df.loc[df['max_wallet'] == 'TD', 'max_tx']\n",
    "\n",
    "df['marketcap'] = df['marketcap'].replace({'\\$': '', ',': ''}, regex=True)\n",
    "df[to_numeric] = df[to_numeric].replace({'%': ''}, regex=True)\n",
    "\n",
    "\n",
    "df[to_numeric] = df[to_numeric].apply(pd.to_numeric)\n",
    "df['marketcap'] = df['marketcap'].apply(pd.to_numeric)\n",
    "\n",
    "# has Eth Îž symbol cannot be parsed by to_numeric()\n",
    "df['funding_amount'] = df['funding_amount'].str.slice(stop=-1).astype(float)\n",
    "df['deployer_balance'] = df['deployer_balance'] .str.slice(stop=-1).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2efae-286d-4f1b-80bf-6827cca1f7fb",
   "metadata": {},
   "source": [
    "Now that numerics have been handled, let's handle any new nulls from our cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1681b4-59c9-429c-bc3c-d1e068aab3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_columns = df.columns[df.isin([np.inf, -np.inf]).any()]\n",
    "\n",
    "for col in inf_columns:\n",
    "    inf_values_count = df[col].isin([np.inf, -np.inf]).sum()\n",
    "    print(f\"Column '{col}' has {inf_values_count} infinite values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4002003-2935-4266-bc68-bcb7d4476f77",
   "metadata": {},
   "source": [
    "Tax being null is the same as there being no tax, therefore tax = 0 and rating is green as we see below in other instances where buy and sell tax are both 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf33ecf7-0eb2-4bb9-bff5-17d3e33d08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['buytax'].isnull() & df['selltax'].isnull(), 'taxrating'] = 'green'\n",
    "df['buytax'].fillna(0, inplace=True)\n",
    "df['selltax'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df209e3d-1aa0-475d-b758-e57ac2b16f84",
   "metadata": {},
   "source": [
    "maximum limitations on max buy and sell means that there is no limit on the amount you can buy, therefore you can theoretically purchase 100% of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47664051-aa1e-4f8c-a389-a7ca0564a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_wallet'].fillna(100, inplace=True)\n",
    "df['max_tx'].fillna(100, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5944223-9ba1-4b05-a530-2cc21df1782c",
   "metadata": {},
   "source": [
    "Let's also convert honeypot to boolean, if the sell is red then it is a honeypot scam and should NOT be bought, and will be given a value of False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2f245f-2e3f-47f5-8435-5d338ec077dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['honeypot'] = np.where(df['honeypot'] == 'True', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ca575-d885-40f3-b437-0f5595d15951",
   "metadata": {},
   "source": [
    "of the non-numeric values, it appears we can hot-encode the following columns:\n",
    "- renounced\n",
    "- buysell_rating\n",
    "- taxrating\n",
    "- owner\n",
    "- owner_rating\n",
    "- funding_source_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b4cb20-c844-42f7-bf03-997006a44b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_dummy = ['renounced', 'buysell_rating', 'taxrating', 'owner', 'owner_rating', 'funding_source_rating']\n",
    "dummies = pd.get_dummies(df[cols_to_dummy])\n",
    "df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a2c13-97bb-4759-8a24-626cac690196",
   "metadata": {},
   "source": [
    "## Finding our Target Class<a id=\"targetclass\"></a>\r\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17f0ba-f1ee-4c0e-b95c-e5df780c7503",
   "metadata": {},
   "source": [
    "Let's add some updated data to our existing dataframe in order to better find our target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83912f41-07a4-4b9d-ad33-865b89cb6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alert = pd.read_csv('../data/partial_discord_data_time_alert.csv')\n",
    "df_alert.drop(columns=['created', 'ath_date'], inplace=True)\n",
    "df_alert = df_alert.drop_duplicates(subset='address', keep='first')\n",
    "df = pd.merge(df, df_alert, on='address', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe521b8-5ba3-4a97-8b22-1c5cf5050133",
   "metadata": {},
   "source": [
    "Alert timestamp needs to be updated to a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3023ed2-eeba-4a5e-b7e6-b793131392d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alert_timestamp'] = pd.to_datetime(df['alert_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97985a9f-eaba-4d26-b419-c09697628740",
   "metadata": {},
   "source": [
    "Now to calculate our measure of success, or target variable \n",
    "\n",
    "Were going to use the all time high price * token supply to get the all time high marketcap\n",
    "\n",
    "Were then going to compare marketcap at liquidity lock to all time high marketcap to measure growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2f2bcf-3614-4706-8dd4-e4ed54c1d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ath_marketcap'] = df['ath_price'] * df['totalSupply']\n",
    "df['growth'] = (df['ath_marketcap'] / df['marketcap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5d86c-2546-494f-882b-67814c5d3aec",
   "metadata": {},
   "source": [
    "As a standard for our initial target class, let's also look at the number of tokens whose ATH date occured at least one day after the liquidity alert time:\r\n",
    "\r\n",
    "Let'observe how various thresholds of minimum time between alert and ath effect the number of potential target class candidates in our dataup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9d6f2e-f0b2-4a92-9ef9-6d1ee4d983e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens that has an ATH date at least 1 days after liquidity lock time are: 2408\n",
      "The number of tokens that has an ATH date at least 2 days after liquidity lock time are: 1914\n",
      "The number of tokens that has an ATH date at least 3 days after liquidity lock time are: 1615\n",
      "The number of tokens that has an ATH date at least 4 days after liquidity lock time are: 1441\n",
      "The number of tokens that has an ATH date at least 5 days after liquidity lock time are: 1311\n",
      "The number of tokens that has an ATH date at least 6 days after liquidity lock time are: 1195\n",
      "The number of tokens that has an ATH date at least 7 days after liquidity lock time are: 1124\n",
      "The number of tokens that has an ATH date at least 8 days after liquidity lock time are: 1054\n",
      "The number of tokens that has an ATH date at least 9 days after liquidity lock time are: 989\n",
      "The number of tokens that has an ATH date at least 10 days after liquidity lock time are: 946\n",
      "The number of tokens that has an ATH date at least 11 days after liquidity lock time are: 909\n",
      "The number of tokens that has an ATH date at least 12 days after liquidity lock time are: 864\n",
      "The number of tokens that has an ATH date at least 13 days after liquidity lock time are: 835\n",
      "The number of tokens that has an ATH date at least 14 days after liquidity lock time are: 787\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    print(f'The number of tokens that has an ATH date at least {i} days after liquidity lock time are:', len(df[(df['ath_date'] - df['alert_timestamp']).dt.days > i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794797e6-82e8-4d72-bafb-ab54b96e98aa",
   "metadata": {},
   "source": [
    "We need to verify our target variables using daily volume: let's take these 2408 tokens and begin a new scrape for that data:\n",
    "\n",
    "Due to resource limitations, we are only scraping for tokens with a gap greater than 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e991e2c5-bb56-440e-b30e-190c3f400acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_volume = df[(df['ath_date'] - df['alert_timestamp']).dt.days > 1][['address', 'alert_timestamp', 'ath_date']]\n",
    "df_for_volume.to_csv('../data/volume_for_target_addresses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01771d2f-bbe3-4eb4-bf2f-1702645ea21b",
   "metadata": {},
   "source": [
    "## Finding Additional Data<a id=\"moreinfo\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97444b07-2cd6-4e5b-ac97-115256d6b7e0",
   "metadata": {},
   "source": [
    "To successfully determine our target class we will need to filter out scam growth and other suspicious price action.\n",
    "\n",
    "A common scam in these tokens is called a 'rug pull'. This is when the token developer removes liquidity from the token, which does not allow anyone to buy or sell their tokens. \n",
    "\n",
    "Even though there are liquidity lock methods, developers have gotten crafty in working around these. As a general rule of thumb, locked liquidity is better than unlocked liquidity, but you should always proceed with caution, as the security of your tokens are never 100% secure. This is one drawback of crypto's anonymous nature. \n",
    "\n",
    "In a rug pull, the developer still has permission to trade tokens, and they well often sell off any remaining tokens they hold in whatever liquidity is left over. Because the remaining liquidity gets used in that last transaction, the price impact is quite high. This means that in one small transaction the developer makes it appear that the token has shot up 100%-1000% in many cases. This shows up in our scraped data for all-time high price.\n",
    "\n",
    "We can return to Defined, where we can get volume metrics. Once liquidity has been pulled, trading can no longer happen, which means volume must be 0 or unusually low. By finding volume metrics for the data of the all time high price, as well as the day before and after, we can get a much more accurate sense of which tokens truly deserve to be in our target class.\n",
    "\n",
    "This scraping process can be found here: **['defined_volume_scrape'](../dataScrape/defined_volume_scrape.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66648e-4212-4cc1-926f-8bdd7e6f995c",
   "metadata": {},
   "source": [
    "We are back from the scrape\n",
    "\n",
    "Let's read in our scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae50ac48-e0c4-488f-862a-dfda4eb7523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_volume = pd.read_csv('../data/target_addresses_volume.csv')\n",
    "df = pd.merge(df, df_with_volume, on='address', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13f8e-e5cc-4709-9a83-557838368aff",
   "metadata": {},
   "source": [
    "Overall look at big gainer tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0040eb-cc16-4cf7-9263-c14ce6f00a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18340\n",
      "8479\n",
      "7092\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df['growth'] > 10]))\n",
    "print(len(df[df['growth'] > 100]))\n",
    "print(len(df[df['growth'] > 1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4a0f1-f01a-42a0-8ecf-c7a4b972281c",
   "metadata": {},
   "source": [
    "Due to domain knowledge, given the size of our dataset we know these numbers are far too high.\n",
    "\n",
    "We need to filter down by more than just date, we should add volume metrics as well to ensure they are truly target class worthy\n",
    "\n",
    "Let's create our target class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f4e87be-f862-40a7-8dd2-9033d913d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['suitable_investment'] = 0\n",
    "\n",
    "# conditions are subject to change beyond the scope of this bootcamp when introducing reinforcement learning\n",
    "conditions = (df['growth'] > 15) & \\\n",
    "              ((df['ath_date'] - df['alert_timestamp']).dt.days > 1) & \\\n",
    "              (df['daily_volume_after_ath'] > 10000) & \\\n",
    "              (df['daily_volume_before_ath'] > 10000) & \\\n",
    "              (df['daily_volume_at_ath'] > 10000)\n",
    "\n",
    "df.loc[conditions, 'suitable_investment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a215d18f-1235-410a-9d33-400ec631e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop = df[(df['suitable_investment'] == 1) & np.isinf(df['growth'])].index\n",
    "df = df.drop(index_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1ab6a6-fa41-470b-b416-29d7e4d42c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['suitable_investment'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394c1fb-a7fc-4337-9c6f-012a4332fb93",
   "metadata": {},
   "source": [
    "We have 665 suitable investments according to our first iteration of requirments\n",
    "\n",
    "Let's export for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db02a1-3067-491b-a2ca-adbe2428b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/complete_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffd785-7f1c-44d6-80de-103af049a97c",
   "metadata": {},
   "source": [
    "## Cleaning Conclusion<a id=\"nextsteps\"></a>\r\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd67bd0-05fb-43fb-b110-3b9587d41d17",
   "metadata": {},
   "source": [
    "We now have some data to work with.\n",
    "\n",
    "Let's get to our data exploration here: **['data_eda'](./data_eda.ipynb)**\n",
    "\n",
    "**Note for the future of this project** \n",
    "\n",
    "We will use this data for the initial iteration of insights and modelling. If we can determine that a profitable trading strategy is indeed possible we will circle back and invest more in our data collection and engineering process. Why?\n",
    "\n",
    "As is stands our trading strategy is based on many conservative assumptions:\n",
    "- We assume a non target class investment results in a 100% loss\n",
    "    - In the real world, while many may be scams there are still growth opportunities between 2-15x, or even breakeven\n",
    "- We assume target class investments cash out at 15x\n",
    "    - While the average growth within our target class is greater than 15, we have not created a selling/take profit strategy, and it would be dishonest to assume that we could sell the abosulte peak of every toke, every time.\n",
    "- Gas fees, liquidity, and transaction limits are factors in modelling, but not factored when evaluation profitability\n",
    "    - When the selling strategy is created, these must be accounted for\n",
    " \n",
    "Creating specific trading strategies requires extensive data, which will likely include scraping the blockchain itself. As it stands now we have a snapshot of the beginning and peak of each token, but to truly create a thorough strategy we need all the trading volume and moves inbetween. \n",
    "\n",
    "Before we invest such resouces in aquiring this data, we need a baseline case to determine if this asset class of crypto is profitable at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414e555-cf48-4d66-9018-25d0f9b1ea07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
