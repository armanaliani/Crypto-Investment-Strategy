{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88d6bc1-5a45-4d2a-bf18-0802da915d96",
   "metadata": {},
   "source": [
    "# Neural Networks for Tokens data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8a0b5-fb33-48ab-b09a-75d1dd358002",
   "metadata": {},
   "source": [
    "We saw that most of our base models did not perform well in terms of precision of the target class.\n",
    "\n",
    "Neural Networks are a powerful machine learning tool, we expect them to outperform the base models in our use case.\n",
    "\n",
    "We saw that we had extremely low coefficients with our target class in our eda so we already knew we needed an in depth and powerful model. Neural Networks are great at capturing complex and non-linear relationships. However, with their power comes a slight drawback, the element of random chance. \n",
    "\n",
    "We've taken this into account, and made precautions to prevent both overfitting and a one off \"lucky\" successful model.\n",
    "\n",
    "While conducting a test/train split is a good method in ensuring a model performs well with unseen data, we took extra precaution. We tested our models both on raw data as well as pca data and ensured the results were within a reasonable range. A good model should detect the underlying pattern in any representation of the same data. PCA helps mitigate collinearity within our data as well as overfitting to the raw train data.\n",
    "\n",
    "We also took an aggregate measure of our model evaluations. Each models architecure was trained multiple times on multiple variations of data. Every single run, for every single model architecure regardless if it was pca data or raw data, was profitable at one or more thresholds. This validates all of our model architures, and suggests in a real world setting we can expect results within our range of results. Therefore we can rule out profitability being due to luck and overfitting. \n",
    "\n",
    "Table of Contents:<a id=\"Contents\"></a>\n",
    "* [Base Model Architecture](#model1)\n",
    "* [Second Model Architecture](#model2)\n",
    "* [Complex Model Architecture](#model3)\n",
    "* [Model Evaluation](#modeleval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99f9b95-174c-482f-94c4-d538d9db9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "# Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee8cb9f-0326-44be-9ff1-5ac32996215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/complete_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43afde2-387a-4c4b-9750-088109faec35",
   "metadata": {},
   "source": [
    "We must remove metrics from our X that were used to determine the target class and will not be avaiable to us when predicting new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31ca020-19d0-4c35-be3d-c1e64f8778d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['suitable_investment', 'ath_marketcap', 'growth', 'ath_price']).select_dtypes(include=['number', 'bool'])\n",
    "y = df['suitable_investment']\n",
    "\n",
    "X = X.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3571acdc-8962-4647-bece-ff052c10b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our target class accounts for 0.011168557935495423% of our train data\n",
      "Our target class accounts for 0.01114617605586517% of our test data\n"
     ]
    }
   ],
   "source": [
    "print(f'Our target class accounts for {y_train.sum() / len(y_train)}% of our train data')\n",
    "print(f'Our target class accounts for {y_test.sum() / len(y_test)}% of our test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f885b95-0a67-4040-b5fd-2ecbf1130859",
   "metadata": {},
   "source": [
    "As an extra measure of model validity, we're going to test the model on original data, pca data, and collinearity adjusted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c7a665-7c80-44c4-b38e-1bffd4648988",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_PCA = PCA()\n",
    "my_PCA.fit(X_train)\n",
    "X_train_PCA = my_PCA.transform(X_train)\n",
    "X_test_PCA = my_PCA.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3079dc8-9673-4f98-a9c4-eccdcbbc6a3f",
   "metadata": {},
   "source": [
    "Given our imbalanced data, we need to ensure we are running our neural network with weighted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798056f7-0087-435a-9130-34e0a2bcf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf8b82-506d-42c0-ade1-d77c8e070ecd",
   "metadata": {},
   "source": [
    "# Model 1<a id=\"model1\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d676fe2-22cd-409f-8b85-f5ed5f0bb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(123)\n",
    "\n",
    "model1_5pca = keras.Sequential()\n",
    "\n",
    "regularizer1 = keras.regularizers.l2(0.001)\n",
    "\n",
    "model1_5pca.add(layers.BatchNormalization())\n",
    "model1_5pca.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.Dropout(0.3))\n",
    "model1_5pca.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.BatchNormalization())\n",
    "model1_5pca.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.Dropout(0.3))\n",
    "model1_5pca.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.BatchNormalization())\n",
    "model1_5pca.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.Dropout(0.3))\n",
    "model1_5pca.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.BatchNormalization())\n",
    "model1_5pca.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "model1_5pca.add(layers.Dense(8, activation=\"relu\", kernel_regularizer=regularizer1))\n",
    "\n",
    "model1_5pca.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model1_5pca.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e450c540-f4b2-4e16-a8e8-cece8b61b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e6ef520760>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 epochs\n",
    "model1_5pca.fit(X_train_PCA, y_train, epochs=25, verbose=0, class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccd5bd-31e1-497c-abf4-1bedef4761f8",
   "metadata": {},
   "source": [
    "Threshold optimization to maximize precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "68d625e6-b535-4193-909c-4f2c8be1b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 0s 642us/step\n",
      "0.7\n",
      "[[14285   442]\n",
      " [  140    26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     14727\n",
      "           1       0.06      0.16      0.08       166\n",
      "\n",
      "    accuracy                           0.96     14893\n",
      "   macro avg       0.52      0.56      0.53     14893\n",
      "weighted avg       0.98      0.96      0.97     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.75\n",
      "[[14524   203]\n",
      " [  150    16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.07      0.10      0.08       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.54      0.54     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8\n",
      "[[14619   108]\n",
      " [  157     9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.08      0.05      0.06       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.52      0.53     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8500000000000001\n",
      "[[14665    62]\n",
      " [  159     7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     14727\n",
      "           1       0.10      0.04      0.06       166\n",
      "\n",
      "    accuracy                           0.99     14893\n",
      "   macro avg       0.55      0.52      0.53     14893\n",
      "weighted avg       0.98      0.99      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.9000000000000001\n",
      "[[14699    28]\n",
      " [  162     4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     14727\n",
      "           1       0.12      0.02      0.04       166\n",
      "\n",
      "    accuracy                           0.99     14893\n",
      "   macro avg       0.56      0.51      0.52     14893\n",
      "weighted avg       0.98      0.99      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1_5pca.predict(X_test_PCA)\n",
    "\n",
    "for threshold in np.arange(0.7, 0.9, 0.05):\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate model performance at each threshold\n",
    "    print(threshold)\n",
    "    print(confusion_matrix(y_test, y_pred_binary))\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecad295-f67d-4b3f-8591-5a92c836c146",
   "metadata": {},
   "source": [
    "#### Model 1 findings \n",
    "tp/fp @ threshold (precision, recall):\n",
    "\n",
    "original data (25 epochs):\n",
    "1. model1_1 | 13/161 @ .7 (7, 8) ----  6/61 @ .75 (9, 4) ---- 1/28 @ .8 (3, 1) ---- 0/10 @ .85 (0, 0)\n",
    "2. model1_2 | 17/240 @ .7 (7, 10) ----  7/91 @ .75 (7, 4) ---- 3/49 @ .8 (6, 2) ---- 0/16 @ .85 (0, 0)\n",
    "3. model1_3 | 27/490 @ .7 (5, 16) ---- 13/156 @ .75 (8, 8) ---- 7/70 @ .8 (9, 4) ---- 3/27 @ .85 (10, 2)\n",
    "4. model1_4 | 18/306 @ .7 (6, 11) ---- 9/78 @ .75 (10, 5) ---- 1/33 @ .8 (3, 1) ---- 0/10 @ .85 (0, 0)\n",
    "5. model1_5 | 18/244 @ .7 (7, 11) ---- 7/60 @ .75 (10, 4) ---- 1/21 @ .8 (5, 1) ---- 0/7 @ .85 (0, 0)\n",
    "\n",
    "pca data:\n",
    "1. model1_1pca | 28/448 @ .7 (6, 17) ----  13/206 @ .75 (6, 8) ---- 7/96 @ .8 (7, 4) ---- 5/58 @ .85 (8, 3)\n",
    "2. model1_2pca | 27/466 @ .7 (5, 16) ----  16/213 @ .75 (7, 10) ---- 9/101 @ .8 (8, 5) ---- 5/53 @ .85 (9, 3)\n",
    "3. model1_3pca | 31/525 @ .7 (6, 19) ----  16/181 @ .75 (8, 10) ---- 8/82 @ .8 (9, 5) ---- 5/28 @ .85 (15, 3)\n",
    "4. model1_4pca | 35/753 @ .7 (4, 21) ----  20/258 @ .75 (7, 12) ---- 10/121 @ .8 (8, 6) ---- 6/70 @ .85 (8, 4)\n",
    "5. model1_5pca | 16/203 @ .7 (7, 10) ----  9/108 @ .75 (8, 5) ---- 7/62 @ .8 (10, 4) ---- 4/28 @ .85 (12, 2)\n",
    "\n",
    "Let's find our averages to compare our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "dafd0fd2-5b23-4e59-9e19-cca96dc791c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 with original data at a threshold of 0.75 has an average precision of: 0.0891659549919972 and recall of 0.050602409638554224\n",
      "---\n",
      "Model 1 with pca data at a threshold of 0.85 has an average precision of: 0.10420689917060152 and recall of 0.03012048192771084\n",
      "---\n",
      "Model 1 with pca data at a threshold of 0.80 has an average precision of: 0.08329067779610574 and recall of 0.059271231266529535\n",
      "---\n",
      "Model 1 with pca data at a threshold of 0.75 has an average precision of: 0.07186270466094252 and recall of 0.0891566265060241\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#original data at .75 threshold\n",
    "precision_1_1 = ((6/67) + (7/98) + (13/169) + (9/87) + (7/67)) / 5\n",
    "recall_1_1 = ((6/166) + (7/166) + (13/166) + (9/166) + (7/166)) / 5\n",
    "print(f'Model 1 with original data at a threshold of 0.75 has an average precision of: {precision_1_1} and recall of {recall_1_1}')\n",
    "print('---')\n",
    "\n",
    "#pca data at .85 threshold\n",
    "precision_1_2 = ((5/63) + (5/58) + (5/33) + (6/76) + (4/32)) / 5\n",
    "recall_1_2 = ((5/166) + (5/166) + (5/166) + (6/166) + (4/166)) / 5\n",
    "print(f'Model 1 with pca data at a threshold of 0.85 has an average precision of: {precision_1_2} and recall of {recall_1_2}')\n",
    "print('---')\n",
    "\n",
    "#pca data at .80 threshold\n",
    "precision_1_3 = ((7/103) + (9/110) + (8/90) + (10/131) + (7/69)) / 5\n",
    "recall_1_3 = ((7/166) + (9/166) + (8/82) + (10/166) + (7/166)) / 5\n",
    "print(f'Model 1 with pca data at a threshold of 0.80 has an average precision of: {precision_1_3} and recall of {recall_1_3}')\n",
    "print('---')\n",
    "\n",
    "#pca data at .75 threshold\n",
    "precision_1_4 = ((13/219) + (16/229) + (16/197) + (20/278) + (9/117)) / 5\n",
    "recall_1_4 = ((13/166) + (16/166) + (16/166) + (20/166) + (9/166)) / 5\n",
    "print(f'Model 1 with pca data at a threshold of 0.75 has an average precision of: {precision_1_4} and recall of {recall_1_4}')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167abe8-f53a-4f0a-be56-dbd46fb0beeb",
   "metadata": {},
   "source": [
    "# Model 2<a id=\"model2\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d441181-5c31-4c6c-8351-9386f19de421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(123)\n",
    "\n",
    "model2_8pca = keras.Sequential()\n",
    "\n",
    "regularizer2 = keras.regularizers.l2(0.0001)\n",
    "\n",
    "model2_8pca.add(layers.BatchNormalization())\n",
    "model2_8pca.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dropout(0.3))\n",
    "model2_8pca.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.BatchNormalization())\n",
    "model2_8pca.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dropout(0.3))\n",
    "model2_8pca.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dropout(0.3))\n",
    "model2_8pca.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.BatchNormalization())\n",
    "model2_8pca.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dropout(0.3))\n",
    "model2_8pca.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.BatchNormalization())\n",
    "model2_8pca.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "model2_8pca.add(layers.Dense(8, activation=\"relu\", kernel_regularizer=regularizer2))\n",
    "\n",
    "model2_8pca.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model2_8pca.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "00d44bd7-855a-4184-ba7d-9119474d6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "history2 = model2_8pca.fit(X_train_PCA, y_train, epochs=10, verbose=0, class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a1755506-5dee-4309-9ff5-292379877f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 0s 704us/step\n",
      "0.7\n",
      "[[13861   866]\n",
      " [  126    40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     14727\n",
      "           1       0.04      0.24      0.07       166\n",
      "\n",
      "    accuracy                           0.93     14893\n",
      "   macro avg       0.52      0.59      0.52     14893\n",
      "weighted avg       0.98      0.93      0.96     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.7500001\n",
      "[[14263   464]\n",
      " [  138    28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     14727\n",
      "           1       0.06      0.17      0.09       166\n",
      "\n",
      "    accuracy                           0.96     14893\n",
      "   macro avg       0.52      0.57      0.53     14893\n",
      "weighted avg       0.98      0.96      0.97     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8000001999999999\n",
      "[[14483   244]\n",
      " [  147    19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     14727\n",
      "           1       0.07      0.11      0.09       166\n",
      "\n",
      "    accuracy                           0.97     14893\n",
      "   macro avg       0.53      0.55      0.54     14893\n",
      "weighted avg       0.98      0.97      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8500002999999999\n",
      "[[14577   150]\n",
      " [  155    11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.07      0.07      0.07       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.53      0.53     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.9000003999999999\n",
      "[[14639    88]\n",
      " [  158     8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.08      0.05      0.06       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.54      0.52      0.53     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model2_8pca.predict(X_test_PCA)\n",
    "\n",
    "for threshold in np.arange(0.7, 0.91, 0.0500001):\n",
    "    y_pred_binary2 = (y_pred2 >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate model performance at each threshold\n",
    "    print(threshold)\n",
    "    print(confusion_matrix(y_test, y_pred_binary2))\n",
    "    print(classification_report(y_test, y_pred_binary2))\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65767da3-4bde-4f8f-9e0f-f1d81c1f5b7a",
   "metadata": {},
   "source": [
    "#### Model 2 findings \n",
    "tp/fp @ threshold (precision, recall):\n",
    "\n",
    "original data:\n",
    "1. model2_1 :20 epochs: 27/481 @ .75 (5, 16) ----  13/238 @ .8 (5, 8) ---- 7/100 @ .85 (7, 4) ---- 3/23 @ .9 (12, 2)\n",
    "2. model2_2 :20 epochs: 16/212 @ .75 (7, 10) ----  8/57 @ .8 (12, 5) ---- 1/24 @ .85 (4, 1) ---- 0/8 @ .9 (0, 0)\n",
    "3. model2_3 :20 epochs: 16/231 @ .75 (6, 10) ----  10/88 @ .8 (10, 6) ---- 5/34 @ .85 (13, 3) ---- 1/8 @ .9 (11, 1)\n",
    "4. model2_4 :20 epochs: 19/304 @ .75 (6, 11) ----  13/135 @ .8 (9, 8) ---- 6/54 @ .85 (10, 4) ---- 1/16 @ .9 (6, 1)\n",
    "\n",
    "5. model2_5 :30 epochs: 13/184 @ .75 (7, 8) ----  7/72 @ .8 (9, 4) ---- 2/23 @ .85 (8, 1) ---- 0/8 @ .9 (0, 0)\n",
    "6. model2_9 :30 epochs: 8/151 @ .75 (5, 5) ----  5/59 @ .8 (8, 3) ---- 1/11 @ .85 (8, 1) ---- 0/4 @ .9 (0, 0)\n",
    "7. model2_7 :30 epochs: 12/199 @ .75 (6, 7) ----  8/70 @ .8 (10, 5) ---- 3/19 @ .85 (14, 2) ---- 0/6 @ .9 (0, 0)\n",
    "8. model2_8 :30 epochs: 14/266 @ .75 (5, 8) ----  10/107 @ .8 (9, 6) ---- 6/31 @ .85 (16, 4) ---- 0/10 @ .9 (0, 0)\n",
    "\n",
    "\n",
    "pca data:\n",
    "1. model2_1pca :20 epochs: 20/304 @ .75 (6, 12) ----  10/135 @ .8 (7, 6) ---- 7/63 @ .85 (10, 4) ---- 3/27 @ .9 (10, 2)\n",
    "2. model2_2pca :20 epochs: 22/392 @ .75 (5, 13) ----  14/189 @ .8 (7, 8) ---- 9/91 @ .85 (9, 5) ---- 4/42 @ .9 (9, 2)\n",
    "3. model2_3pca :20 epochs: 21/344 @ .75 (6, 13) ----  14/196 @ .8 (7, 8) ---- 10/108 @ .85 (8, 6) ---- 6/69 @ .9 (8, 4)\n",
    "4. model2_4pca :20 epochs: 36/650 @ .75 (5, 22) ----  23/360 @ .8 (6, 14) ---- 11/178 @ .85 (6, 7) ---- 8/90 @ .9 (8, 5)\n",
    "\n",
    "5. model2_5pca :10 epochs: 25/388 @ .75 (6, 15) ----  15/191 @ .8 (7, 9) ---- 9/104 @ .85 (8, 5) ---- 4/49 @ .9 (8, 2)\n",
    "6. model2_6pca :10 epochs: 32/597 @ .75 (5, 19) ----  21/281 @ .8 (7, 13) ---- 13/150 @ .85 (8, 8) ---- 9/80 @ .9 (10, 5)\n",
    "7. model2_7pca :10 epochs: 33/625 @ .75 (5, 20) ----  23/361 @ .8 (6, 14) ---- 15/193 @ .85 (7, 9) ---- 9/115 @ .9 (7, 5)\n",
    "8. model2_8pca :10 epochs: 28/464 @ .75 (5, 20) ----  19/244 @ .8 (7, 11) ---- 11/150 @ .85 (7, 7) ---- 8/88 @ .9 (8, 5)\n",
    "\n",
    "\n",
    "Let's find our averages to compare our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "01d2abe3-28ff-4022-b816-57835e8c99c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 with original data at a threshold of 0.80 has an average precision of: 0.08993939861218092 and recall of 0.05572289156626506\n",
      "---\n",
      "Model 2 with original data at a threshold of 0.85 has an average precision of: 0.10568560260149044 and recall of 0.023343373493975906\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#original data at .80 threshold\n",
    "precision_2_1 = ((13/251) + (8/65) + (10/98) + (13/148) + (7/79) + (5/64) + (8/78) + (10/117)) / 8\n",
    "recall_2_1 = ((13/166) + (8/166) + (10/166) + (13/166) + (7/166) + (5/166) + (8/166) + (10/166)) / 8\n",
    "print(f'Model 2 with original data at a threshold of 0.80 has an average precision of: {precision_2_1} and recall of {recall_2_1}')\n",
    "print('---')\n",
    "\n",
    "#original data at .85 threshold\n",
    "precision_2_2 = ((7/107) + (1/25) + (5/39) + (6/40) + (2/25) + (1/12) + (3/22) + (6/37)) / 8\n",
    "recall_2_2 = ((7/166) + (1/166) + (5/166) + (6/166) + (2/166) + (1/166) + (3/166) + (6/166)) / 8\n",
    "print(f'Model 2 with original data at a threshold of 0.85 has an average precision of: {precision_2_2} and recall of {recall_2_2}')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b83c42-c587-495d-a821-03c3f7e982d3",
   "metadata": {},
   "source": [
    "# Model 3<a id=\"model3\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "06ff4faa-21ed-48de-8d24-f104573e9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(123)\n",
    "\n",
    "model3_5pca = keras.Sequential()\n",
    "\n",
    "regularizer3 = keras.regularizers.l2(0.0001)\n",
    "\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dense(36, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(72, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(144, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(288, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(576, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(1152, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(576, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(288, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(144, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(72, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(36, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(18, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "model3_5pca.add(layers.Dropout(0.3))\n",
    "model3_5pca.add(layers.Dense(9, activation=\"relu\", kernel_regularizer=regularizer3))\n",
    "model3_5pca.add(layers.BatchNormalization())\n",
    "\n",
    "model3_5pca.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model3_5pca.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9c3afb13-ffc1-4a01-9eb1-e63c1e68ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1397/1397 [==============================] - 21s 13ms/step - loss: 1.1295 - binary_accuracy: 0.4674 - precision_53: 0.0109\n",
      "Epoch 2/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 1.0940 - binary_accuracy: 0.4719 - precision_53: 0.0119\n",
      "Epoch 3/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 1.0916 - binary_accuracy: 0.4419 - precision_53: 0.0123\n",
      "Epoch 4/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 1.0827 - binary_accuracy: 0.5176 - precision_53: 0.0138\n",
      "Epoch 5/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 1.0492 - binary_accuracy: 0.4640 - precision_53: 0.0146\n",
      "Epoch 6/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 1.0161 - binary_accuracy: 0.5222 - precision_53: 0.0151\n",
      "Epoch 7/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 0.9707 - binary_accuracy: 0.5798 - precision_53: 0.0169\n",
      "Epoch 8/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 0.9194 - binary_accuracy: 0.6453 - precision_53: 0.0178\n",
      "Epoch 9/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 0.8819 - binary_accuracy: 0.6224 - precision_53: 0.0183\n",
      "Epoch 10/10\n",
      "1397/1397 [==============================] - 18s 13ms/step - loss: 0.8704 - binary_accuracy: 0.6490 - precision_53: 0.0197\n"
     ]
    }
   ],
   "source": [
    "history3 = model3_5pca.fit(X_train_PCA, y_train, epochs=10, verbose=1, class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ee232e5a-e673-4522-83ec-fb9e315e3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 1s 3ms/step\n",
      "0.7\n",
      "[[14385   342]\n",
      " [  146    20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     14727\n",
      "           1       0.06      0.12      0.08       166\n",
      "\n",
      "    accuracy                           0.97     14893\n",
      "   macro avg       0.52      0.55      0.53     14893\n",
      "weighted avg       0.98      0.97      0.97     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.75\n",
      "[[14544   183]\n",
      " [  153    13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.07      0.08      0.07       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.53      0.53     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8\n",
      "[[14623   104]\n",
      " [  158     8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.07      0.05      0.06       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.52      0.52     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.8500000000000001\n",
      "[[14652    75]\n",
      " [  160     6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14727\n",
      "           1       0.07      0.04      0.05       166\n",
      "\n",
      "    accuracy                           0.98     14893\n",
      "   macro avg       0.53      0.52      0.52     14893\n",
      "weighted avg       0.98      0.98      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n",
      "0.9000000000000001\n",
      "[[14684    43]\n",
      " [  163     3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     14727\n",
      "           1       0.07      0.02      0.03       166\n",
      "\n",
      "    accuracy                           0.99     14893\n",
      "   macro avg       0.53      0.51      0.51     14893\n",
      "weighted avg       0.98      0.99      0.98     14893\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_nn3 = model3_5pca.predict(X_test_PCA)\n",
    "\n",
    "for threshold in np.arange(0.7, 0.91, 0.05):\n",
    "    y_pred_binary3 = (y_pred_nn3 >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate model performance at each threshold\n",
    "    print(threshold)\n",
    "    print(confusion_matrix(y_test, y_pred_binary3))\n",
    "    print(classification_report(y_test, y_pred_binary3))\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759acd2-fcce-4c8c-badd-7946424be4f5",
   "metadata": {},
   "source": [
    "#### Model 3 findings \n",
    "tp/fp @ threshold (precision, recall):\n",
    "\n",
    "original data:\n",
    "1. model3_1 :10 epochs: 14/224 @ .7 (6, 8)  ---- 7/84 @ .75 (8, 4)    ----  7/63 @ .8 (10, 4)   ---- 3/38 @ .85 (7, 2)    ---- 1/28 @ .9 (3, 1)\n",
    "2. model3_2 :10 epochs: 26/438 @ .7 (6, 16) ---- 22/348 @ .75 (6, 13) ----  21/259 @ .8 (7, 13) ---- 17/205 @ .85 (8, 10) ---- 13/134 @ .9 (9, 8)\n",
    "3. model3_3 :10 epochs: 21/308 @ .7 (6, 13) ---- 17/207 @ .75 (8, 10) ----  11/107 @ .8 (9, 7)  ---- 2/37 @ .85 (5, 1)    ---- 0/18 @ .9 (0, 0)\n",
    "4. model3_4 :10 epochs: 30/478 @ .7 (6, 18) ---- 16/222 @ .75 (7, 10) ----  8/96 @ .8 (8, 5)    ---- 4/47 @ .85 (8, 2)    ---- 0/26 @ .9 (0, 0)\n",
    "5. model3_5 :10 epochs: 29/597 @ .7 (5, 17) ---- 26/497 @ .75 (5, 16) ----  24/371 @ .8 (6, 14) ---- 20/265 @ .85 (7, 12) ---- 13/142 @ .9 (8, 8)\n",
    "\n",
    "\n",
    "pca data:\n",
    "1. model3_1pca :10 epochs: 22/309 @ .7 (7, 13)  ---- 14/135 @ .75 (9, 8) ----  7/76 @ .8 (8, 4)  ---- 2/37 @ .85 (5, 1)  ---- 0/14 @ .9 (0, 0)\n",
    "2. model3_2pca :10 epochs: 25/514 @ .7 (5, 15)  ---- 14/222 @ .75 (6, 8) ----  9/115 @ .8 (7, 5) ---- 5/51 @ .85 (9, 3)  ---- 1/21 @ .9 (5, 1)\n",
    "3. model3_3pca :10 epochs: 23/537 @ .7 (4, 14)  ---- 14/292 @ .75 (5, 8) ----  8/136 @ .8 (6, 5) ---- 6/50 @ .85 (11, 4) ---- 3/19 @ .9 (14, 2)\n",
    "4. model3_4pca :10 epochs: 31/758 @ .7 (4, 19)  ---- 13/175 @ .75 (7, 8) ----  8/57 @ .8 (12, 5) ---- 4/24 @ .85 (14, 2) ---- 0/7 @ .9 (0, 0)\n",
    "5. model3_5pca :10 epochs: 20/342 @ .7 (6, 12)  ---- 13/183 @ .75 (7, 8) ----  8/104 @ .8 (7, 5) ---- 6/75 @ .85 (7, 4)  ---- 3/43 @ .9 (7, 2)\n",
    "\n",
    "\n",
    "Let's find our averages to compare our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b6bbc6ad-7642-4331-8bed-e620e6674979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 with original data at a threshold of 0.80 has an average precision of: 0.08118058191540277 and recall of 0.0855421686746988\n",
      "---\n",
      "Model 3 with pca data at a threshold of 0.80 has an average precision of: 0.08139580892398615 and recall of 0.04819277108433735\n",
      "---\n",
      "Model 3 with pca data at a threshold of 0.85 has an average precision of: 0.09292836792836792 and recall of 0.027710843373493978\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#original data at .80 threshold\n",
    "precision_3_1 = ((7/70) + (21/280) + (11/118) + (8/104) + (24/395)) / 5\n",
    "recall_3_1 = ((7/166) + (21/166) + (11/166) + (8/166) + (24/166)) / 5\n",
    "print(f'Model 3 with original data at a threshold of 0.80 has an average precision of: {precision_3_1} and recall of {recall_3_1}')\n",
    "print('---')\n",
    "\n",
    "#pca data at .80 threshold\n",
    "precision_3_2 = ((7/83) + (9/124) + (8/144) + (8/65) + (8/112)) / 5\n",
    "recall_3_2 = ((7/166) + (9/166) + (8/166) + (8/166) + (8/166)) / 5\n",
    "print(f'Model 3 with pca data at a threshold of 0.80 has an average precision of: {precision_3_2} and recall of {recall_3_2}')\n",
    "print('---')\n",
    "\n",
    "#pca data at .85 threshold\n",
    "precision_3_3 = ((2/39) + (5/56) + (6/56) + (4/28) + (6/81)) / 5\n",
    "recall_3_3 = ((2/166) + (5/166) + (6/166) + (4/166) + (6/166)) / 5\n",
    "print(f'Model 3 with pca data at a threshold of 0.85 has an average precision of: {precision_3_3} and recall of {recall_3_3}')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f2684-fcf5-4c3e-aa6f-d6c5c8761670",
   "metadata": {},
   "source": [
    "### The Precision and Recall trade-off<a id=\"modeleval\"></a>\n",
    "<p><a href=\"#Contents\" style=\"font-size: 12px;\">Back to Table of Contents</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df173f1c-db99-4dae-b81c-f4b2239b8d8e",
   "metadata": {},
   "source": [
    "Beyond the scope of this bootcamp, will explore what happens in cases where we are not in the target class. The assumption currently is that it's a scam and you lose all your money, we did not consider the potential of it simply not reaching the minimum growth, or pulling out funds before the rug pulls. Even if there's no profit on these tokens, there is a possibility to approach breakeven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96459b2d-0abd-4090-8f84-91729e9304ec",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "- non target class is a 100% loss\n",
    "- target cashout at 15x\n",
    "- gas fees, liquidity, and transaction limits are non factors\n",
    "- $100 in each token \n",
    "\n",
    "Based on average marketcap and wallet limitations of the target class, we can put up to $1000 in most cases\n",
    "\n",
    "we need a 6.7% precision to break even with these assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d35961-c5d0-4795-88a6-3ccd3144efca",
   "metadata": {},
   "source": [
    "Based on these assumptions, let's decide what our best model is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a1e47-599c-49a8-a5e4-387dd3c50f15",
   "metadata": {},
   "source": [
    "Each investment in a target class investment would return $7500 based on our 15x cashout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d909c52-99e8-4708-9051-05e3ed0e3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_target = 166\n",
    "investment_amount = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f73fbd-2a99-4bd5-b333-bb1e52da4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_profit(precision, recall, total_pos, invested):\n",
    "    tp = recall * total_pos\n",
    "    fp = tp * ((1 / precision) - 1)\n",
    "\n",
    "    spent = round((invested * fp), 2)\n",
    "    made = round(((invested * 15) * tp), 2)\n",
    "    profit = made - spent\n",
    "    multiple = round((made / spent), 2)\n",
    "\n",
    "    print(f'You made ${made} in good investments') \n",
    "    print(f'But lost ${spent} in bad investments')\n",
    "    print(f'For a profit of ${profit} and a portfolio multiple of {multiple}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96172b6-0444-486e-9152-ecd55add4906",
   "metadata": {},
   "source": [
    "**Model Architecure 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "661491b0-abbf-419f-920e-5e8cadf9f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $12600.0 in good investments\n",
      "But lost $8580.64 in bad investments\n",
      "For a profit of $4019.3600000000006 and a portfolio multiple of 1.47x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_1_1, recall_1_1, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7c6ba76c-0929-4b8b-8d92-11677a6d1bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $7500.0 in good investments\n",
      "But lost $4298.15 in bad investments\n",
      "For a profit of $3201.8500000000004 and a portfolio multiple of 1.74x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_1_2, recall_1_2, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "28c41f62-7621-4945-bf08-cbb3f7907aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $14758.54 in good investments\n",
      "But lost $10828.97 in bad investments\n",
      "For a profit of $3929.5700000000015 and a portfolio multiple of 1.36x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_1_3, recall_1_3, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5e001e2f-b5dc-4c8c-a77f-e511e679b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $22200.0 in good investments\n",
      "But lost $19114.83 in bad investments\n",
      "For a profit of $3085.1699999999983 and a portfolio multiple of 1.16x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_1_4, recall_1_4, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad783a8-ab8c-488c-a67b-0d9283fa7df5",
   "metadata": {},
   "source": [
    "**Model Architecure 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f11cbf33-7fdd-41ff-a58e-84c2693045ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $13875.0 in good investments\n",
      "But lost $9359.7 in bad investments\n",
      "For a profit of $4515.299999999999 and a portfolio multiple of 1.48x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_2_1, recall_2_1, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "7aadc08e-1ad4-4046-ae41-7cbec8bdef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $5812.5 in good investments\n",
      "But lost $3279.04 in bad investments\n",
      "For a profit of $2533.46 and a portfolio multiple of 1.77x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_2_2, recall_2_2, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef29b7-5677-45db-a390-ec6fb2401709",
   "metadata": {},
   "source": [
    "**Model Architecure 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f579758b-1467-4d2f-8991-078c712db924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $21300.0 in good investments\n",
      "But lost $16071.87 in bad investments\n",
      "For a profit of $5228.129999999999 and a portfolio multiple of 1.33x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_3_1, recall_3_1, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f4479621-cc47-4381-ae1f-6bf1487c8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $12000.0 in good investments\n",
      "But lost $9028.52 in bad investments\n",
      "For a profit of $2971.4799999999996 and a portfolio multiple of 1.33x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_3_2, recall_3_2, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "77bd2bae-06a5-4f59-b219-9ea84a6b7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $6900.0 in good investments\n",
      "But lost $4490.05 in bad investments\n",
      "For a profit of $2409.95 and a portfolio multiple of 1.54x\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_profit(precision_3_3, recall_3_3, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7bf44b-e8e6-4497-916f-c62609b5f0db",
   "metadata": {},
   "source": [
    "Due to the scarcity of the target class in our data and the element of randomness in neural networks\n",
    "\n",
    "We validated our process by training and testing on multiple data types to ensure it can recognize patterns in any given data\n",
    "\n",
    "and we did an aggregate of the evaluation metrics to provide a more honest range of how the model may perform infont of real world data\n",
    "\n",
    "Our best aggregate model architecture for overall profit was model 3 with $26000, however youll notice the multiple is low, this strategy is higher risk and requires a lot of capital.\n",
    "\n",
    "A more conservative but highly profitable model would be model 1 with a multiple of 1.74x and profit of $16000\n",
    "\n",
    "Our best individual model in terms of precision was 'model2_8' at a 0.85 threshold. Let's take a look at the profitibility for that model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dc7a9d45-a7c2-4612-8c38-d8e9a787a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made $9000.0 in good investments\n",
      "But lost $3100.0 in bad investments\n",
      "For a profit of $5900.0 and a portfolio multiple of 2.9x\n"
     ]
    }
   ],
   "source": [
    "precision_2_8 = 6 / 37\n",
    "recall_2_8 = 6 / 166\n",
    "evaluate_model_profit(precision_2_8, recall_2_8, total_target, investment_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f7832-603e-4139-b0c6-9a76320ae74d",
   "metadata": {},
   "source": [
    "Our best model has a investment porfolio multiple of 2.9x and a profit over $5900\n",
    "\n",
    "These results far surpass our best base model (Optimized Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36958ace-18b8-4899-a9a5-e135c01504c5",
   "metadata": {},
   "source": [
    "Over the course of a year we have identified 665 target class investments... \n",
    "\n",
    "Theoretically our test data set of 166 target class tokens represents 3-4 months of investing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
